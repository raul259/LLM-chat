# ğŸ¤– LLM Chat Pro

AplicaciÃ³n de chat con inteligencia artificial usando Next.js 15, OpenAI y diseÃ±o dark profesional.

![Next.js](https://img.shields.io/badge/Next.js-15.5.6-black)
![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue)
![OpenAI](https://img.shields.io/badge/OpenAI-API-green)
![Tailwind CSS](https://img.shields.io/badge/Tailwind-4.0-cyan)

## âœ¨ CaracterÃ­sticas

- ğŸ’¬ Chat en tiempo real con streaming
- ğŸ¨ DiseÃ±o dark profesional y moderno
- ğŸ”’ Seguridad con rate limiting y validaciÃ³n de inputs
- ğŸ“± Responsive y optimizado para todos los dispositivos
- âš¡ Construido con Next.js 15 y Turbopack
- ğŸ¯ TypeScript para type safety

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/raul259/LLM-chat.git
cd LLM-chat
```

### 2. Instalar dependencias

```bash
npm install
```

### 3. Configurar variables de entorno

Crea un archivo `.env.local` en la raÃ­z del proyecto:

```env
OPENAI_API_KEY=tu_clave_api_de_openai
```

**Â¿CÃ³mo obtener tu API Key de OpenAI?**

1. Ve a [platform.openai.com](https://platform.openai.com/)
2. Crea una cuenta o inicia sesiÃ³n
3. Ve a **API Keys** en el menÃº
4. Click en **Create new secret key**
5. Copia la clave (solo se muestra una vez)
6. PÃ©gala en tu archivo `.env.local`

### 4. Ejecutar en modo desarrollo

```bash
npm run dev
```

Abre [http://localhost:3000](http://localhost:3000) en tu navegador.

## ğŸ“¦ Despliegue en Vercel

### OpciÃ³n 1: Desde la interfaz de Vercel (Recomendado)

1. Ve a [vercel.com](https://vercel.com) e inicia sesiÃ³n con GitHub
2. Click en **Add New Project**
3. Importa el repositorio `raul259/LLM-chat`
4. Configura las variables de entorno:
   - **Key**: `OPENAI_API_KEY`
   - **Value**: Tu API key de OpenAI
   - **Environments**: Marca Production, Preview y Development
5. Click en **Deploy**

### OpciÃ³n 2: Con Vercel CLI

```bash
# Instalar Vercel CLI
npm i -g vercel

# Login
vercel login

# Desplegar
vercel

# Agregar variable de entorno
vercel env add OPENAI_API_KEY

# ProducciÃ³n
vercel --prod
```

## ğŸ”§ ConfiguraciÃ³n

### Modelos disponibles

Por defecto usa `gpt-4o-mini`. Puedes cambiarlo en `components/llmchat.tsx`:

```typescript
const [model] = React.useState("gpt-4o-mini");
// Otras opciones: "gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"
```

### Rate Limiting

Configurado en `app/api/llm/route.ts`:

```typescript
const MAX_MESSAGE_LENGTH = 2000; // Caracteres por mensaje
const MAX_MESSAGES = 20;         // Mensajes en conversaciÃ³n
// 10 requests por minuto por IP
```

### Personalizar el prompt del sistema

Edita en `components/llmchat.tsx`:

```typescript
const [messages, setMessages] = React.useState<UIMessage[]>([
  {
    id: uid(),
    role: "developer",
    content: "Eres un asistente Ãºtil. Responde claro y directo.",
  },
]);
```

## ğŸ—ï¸ Estructura del proyecto

```
llm/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ llm/
â”‚   â”‚       â””â”€â”€ route.ts       # API endpoint para OpenAI
â”‚   â”œâ”€â”€ globals.css            # Estilos globales
â”‚   â”œâ”€â”€ layout.tsx             # Layout principal
â”‚   â””â”€â”€ page.tsx               # PÃ¡gina principal
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ llmchat.tsx            # Componente principal del chat
â”‚   â””â”€â”€ ui/                    # Componentes UI (shadcn)
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ openai.ts              # Cliente de OpenAI
â”‚   â””â”€â”€ utils.ts               # Utilidades
â”œâ”€â”€ .env.local                 # Variables de entorno (NO COMMITEAR)
â”œâ”€â”€ next.config.ts             # ConfiguraciÃ³n de Next.js
â”œâ”€â”€ package.json               # Dependencias
â””â”€â”€ tsconfig.json              # ConfiguraciÃ³n TypeScript
```

## ğŸ›¡ï¸ Seguridad

### Protecciones implementadas

âœ… **Rate Limiting**: 10 requests/minuto por IP  
âœ… **ValidaciÃ³n de inputs**: LÃ­mite de caracteres y mensajes  
âœ… **SanitizaciÃ³n**: Renderizado como texto plano, sin HTML  
âœ… **Variables de entorno**: API key nunca expuesta al cliente  
âœ… **Error handling**: Manejo robusto de errores

### Recomendaciones adicionales para producciÃ³n

- ğŸ” Implementar autenticaciÃ³n (NextAuth, Clerk, Auth0)
- ğŸ“Š Monitorear uso y costos en OpenAI Dashboard
- ğŸš¦ Usar Redis/Upstash para rate limiting profesional
- ğŸ” Implementar OpenAI Moderation API
- ğŸ“ Logging y analytics (Vercel Analytics, Posthog)

## ğŸ› SoluciÃ³n de problemas

### Error: "Missing OPENAI_API_KEY"

**Causa**: La variable de entorno no estÃ¡ configurada.

**SoluciÃ³n**:
1. Verifica que `.env.local` existe y tiene la key
2. En Vercel: Settings â†’ Environment Variables â†’ Agregar `OPENAI_API_KEY`
3. Redeploy despuÃ©s de agregar la variable

### Error: "Failed to collect page data"

**Causa**: Build falla al intentar ejecutar cÃ³digo que requiere la API key.

**SoluciÃ³n**: Ya estÃ¡ solucionado en el cÃ³digo actual. La validaciÃ³n de API key solo ocurre en runtime.

### Error en Vercel: "EISDIR: illegal operation"

**Causa**: Problema con Windows y rutas con espacios.

**SoluciÃ³n**: El modo dev funciona perfectamente. Para build local, mueve el proyecto a una ruta sin espacios.

### Rate limit alcanzado

**SÃ­ntoma**: Error 429 "Too many requests"

**SoluciÃ³n**: Espera 1 minuto. El lÃ­mite actual es 10 requests/minuto por IP.

## ğŸ“š Stack tecnolÃ³gico

- **Framework**: [Next.js 15](https://nextjs.org/)
- **Lenguaje**: [TypeScript](https://www.typescriptlang.org/)
- **Estilos**: [Tailwind CSS 4](https://tailwindcss.com/)
- **Componentes UI**: [shadcn/ui](https://ui.shadcn.com/)
- **IA**: [OpenAI API](https://platform.openai.com/)
- **Iconos**: [Lucide React](https://lucide.dev/)
- **Deployment**: [Vercel](https://vercel.com/)

## ğŸ“ Scripts disponibles

```bash
npm run dev      # Modo desarrollo con Turbopack
npm run build    # Build para producciÃ³n
npm run start    # Iniciar servidor de producciÃ³n
npm run lint     # Ejecutar ESLint
```

## ğŸ¤ Contribuir

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

## ğŸ‘¨â€ğŸ’» Autor

**raul259**  
GitHub: [@raul259](https://github.com/raul259)

## ğŸ™ Agradecimientos

- [OpenAI](https://openai.com/) por su increÃ­ble API
- [Vercel](https://vercel.com/) por el hosting gratuito
- [shadcn](https://ui.shadcn.com/) por los componentes UI

---

â­ Si te gusta este proyecto, dale una estrella en GitHub!
